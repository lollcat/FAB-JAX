optimizer:
    init_lr: 2e-5
    optimizer_name: adam
    use_schedule: true
    peak_lr: 2e-4
    end_lr: 2e-5
    warmup_n_epoch: 10
    max_global_norm: null
    max_param_grad: null
    dynamic_grad_ignore_and_clip: true
    dynamic_grad_ignore_factor: 10. # When to fully ignore gradient.
    dynamic_grad_norm_factor: 2. # When to clip gradient norm.
    dynamic_grad_norm_window: 100 # Window to track median gradient norm over.
n_epoch: 1000
batch_size: 128
eval_batch_size: 2_000
plot_batch_size: 1_000
use_64_bit: false
seed: 0
n_checkpoints: 0
n_eval: 20
save_in_wandb_dir: true
save_dir: ""
